{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### Import Section\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import env\n",
    "import wrangle_zillow\n",
    "from os.path import exists\n",
    "\n",
    "from itertools import product\n",
    "from scipy.stats import levene , pearsonr, spearmanr, mannwhitneyu, f_oneway, ttest_ind\n",
    "from sklearn.metrics import mean_squared_error, explained_variance_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.linear_model import LinearRegression, TweedieRegressor, LassoLars\n",
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "from sklearn.feature_selection import RFE, f_regression, SelectKBest\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !!!!!!!! WRITE UP A MODULE DESCRIPTION\n",
    "\n",
    "def get_connection(db, user=env.user, host=env.host, password=env.password):\n",
    "    return f'mysql+pymysql://{user}:{password}@{host}/{db}'\n",
    "\n",
    "def get_zillow_data():\n",
    "    '''\n",
    "    Reads in all fields from the customers table in the mall_customers schema from data.codeup.com\n",
    "    \n",
    "    parameters: None\n",
    "    \n",
    "    returns: a single Pandas DataFrame with the index set to the primary customer_id field\n",
    "    '''\n",
    "\n",
    "    sql = \"\"\"\n",
    "    SELECT \n",
    "        prop.*,\n",
    "        predictions_2017.logerror as logerror,\n",
    "        aircon.airconditioningdesc as aircon,\n",
    "        arch.architecturalstyledesc as architecture,\n",
    "        buildclass.buildingclassdesc as building_class, \n",
    "        heating.heatingorsystemdesc as heating,\n",
    "        landuse.propertylandusedesc as landuse, \n",
    "        story.storydesc as story,\n",
    "        construct_type.typeconstructiondesc as construct_type\n",
    "    FROM properties_2017 prop\n",
    "    JOIN (\n",
    "        SELECT parcelid, MAX(transactiondate) AS max_transactiondate\n",
    "        FROM predictions_2017\n",
    "        GROUP BY parcelid\n",
    "        ) pred USING (parcelid)\n",
    "    JOIN predictions_2017 ON pred.parcelid = predictions_2017.parcelid\n",
    "                        AND pred.max_transactiondate = predictions_2017.transactiondate\n",
    "    LEFT JOIN propertylandusetype landuse USING (propertylandusetypeid)\n",
    "    LEFT JOIN airconditioningtype aircon USING (airconditioningtypeid)\n",
    "    LEFT JOIN architecturalstyletype arch USING (architecturalstyletypeid)\n",
    "    LEFT JOIN buildingclasstype buildclass USING (buildingclasstypeid)\n",
    "    LEFT JOIN heatingorsystemtype heating USING (heatingorsystemtypeid)\n",
    "    LEFT JOIN storytype story USING (storytypeid)\n",
    "    LEFT JOIN typeconstructiontype construct_type USING (typeconstructiontypeid)\n",
    "    WHERE propertylandusedesc IN (\"Single Family Residential\", \"Inferred Single Family Residential\") \n",
    "        AND transactiondate like '%%2017%%';\n",
    "    \"\"\"\n",
    "\n",
    "    if exists('zillow_data.csv'):\n",
    "        df = pd.read_csv('zillow_data.csv')\n",
    "    else:\n",
    "        df = pd.read_sql(sql, get_connection('zillow'))\n",
    "        df.to_csv('zillow.csv', index=False)\n",
    "    return df\n",
    "\n",
    "def nulls_by_col(df):\n",
    "    num_missing = df.isnull().sum()\n",
    "    percnt_miss = num_missing / df.shape[0] * 100\n",
    "    cols_missing = pd.DataFrame(\n",
    "        {\n",
    "            'num_rows_missing': num_missing,\n",
    "            'percent_rows_missing': percnt_miss\n",
    "        }\n",
    "    )\n",
    "    return cols_missing\n",
    "\n",
    "def nulls_by_row(df):\n",
    "    num_missing = df.isnull().sum(axis=1)\n",
    "    prnt_miss = num_missing / df.shape[1] * 100\n",
    "    rows_missing = pd.DataFrame({'num_cols_missing': num_missing, 'percent_cols_missing': prnt_miss})\n",
    "    rows_missing = rows_missing.reset_index().groupby(['num_cols_missing', 'percent_cols_missing']).count().reset_index()\n",
    "\n",
    "    return rows_missing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def engineer_features(df):\n",
    "    \"\"\"\n",
    "    \"\"\"\n",
    "\n",
    "    #remove unwanted columns, and reset index to id --> for the exercises\n",
    "    #age\n",
    "    df['age'] = 2022 - df['yearbuilt']\n",
    "\n",
    "    #log error bin\n",
    "    df['logerror_bin'] = pd.cut(df.logerror,[-6, df.logerror.mean() - df.logerror.std(), \n",
    "                            df.logerror.mean() + df.logerror.std(), 10],labels=['<-1sig','-1sig~1sig','>1sig'])\n",
    "    \n",
    "    #rename \n",
    "    df = df.rename(columns={'fips': 'county',\n",
    "                            'bedroomcnt': 'bedrooms', \n",
    "                            'bathroomcnt':'bathrooms', \n",
    "                            'calculatedfinishedsquarefeet': 'area', \n",
    "                            'taxvaluedollarcnt': 'home_value',\n",
    "                            'yearbuilt': 'year_built', \n",
    "                            'taxamount': 'tax_amount', \n",
    "                            })\n",
    "    \n",
    "    # #### Decades: \n",
    "    # #create list to hold labels for decades\n",
    "    # decade_labels = [x + 's' for x in np.arange(1870, 2030, 10)[:-1].astype('str')]\n",
    "\n",
    "    # #assign decades created from range to new decades column in dataset and apply labels\n",
    "    # df['decades'] = pd.cut(df.year_built, np.arange(1870, 2030, 10), labels=decade_labels, ordered=True)\n",
    "\n",
    "    #### Home Size\n",
    "    #use quantiles to calculate subgroups and assign to new column\n",
    "    q1, q3 = df.area.quantile([.25, .75])\n",
    "    df['home_size'] = pd.cut(df.area, [0,q1,q3, df.area.max()], labels=['small', 'medium', 'large'], right=True)\n",
    "\n",
    "    #### Estimated Tax Rate\n",
    "    df['est_tax_rate'] = df.tax_amount / df.home_value\n",
    "\n",
    "    df.county = df.county.map({6037: 'LA County', 6059: 'Orange County', 6111: 'Ventura County'})\n",
    "\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def summarize(df):\n",
    "    print('-----')\n",
    "    print('DataFrame info:\\n')\n",
    "    print (df.info())\n",
    "    print('---')\n",
    "    print('DataFrame describe:\\n')\n",
    "    print (df.describe())\n",
    "    print('---')\n",
    "    print('DataFrame null value asssessment:\\n')\n",
    "    print('Nulls By Column:', nulls_by_col(df))\n",
    "    print('----')\n",
    "    print('Nulls By Row:', nulls_by_row(df))\n",
    "    numerical_cols = df.select_dtypes(include='number').columns.to_list()\n",
    "    categorical_cols = df.select_dtypes(exclude='number').columns.to_list()\n",
    "    print('value_counts: \\n')\n",
    "    for col in df.columns:\n",
    "        print(f'Column Names: {col}')\n",
    "        if col in categorical_cols:\n",
    "            print(df[col].value_counts())\n",
    "        else:\n",
    "            print(df[col].value_counts(bins=10, sort=False, dropna=False))\n",
    "            print('---')\n",
    "    print('Report Finished')\n",
    "    return\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def handle_missing_values(df, prop_required_columns=0.60, prop_required_row=0.75):\n",
    "    threshold = int(round(prop_required_columns * len(df.index), 0))\n",
    "    df = df.dropna(axis=1, thresh=threshold)\n",
    "    threshold = int(round(prop_required_row * len(df.columns), 0))\n",
    "    df = df.dropna(axis=0, thresh=threshold)\n",
    "\n",
    "    return df\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_data(df):\n",
    "    train_validate, test = train_test_split(df, test_size= .2, random_state=514)\n",
    "    train, validate = train_test_split(train_validate, test_size= .3, random_state=514)\n",
    "    print(train.shape, validate.shape, test.shape)\n",
    "    return train, validate, test\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def scale_split_data (train, validate, test):\n",
    "    #create scaler object\n",
    "    scaler = MinMaxScaler()\n",
    "\n",
    "    # create copies to hold scaled data\n",
    "    train_scaled = train.copy(deep=True)\n",
    "    validate_scaled = validate.copy(deep=True)\n",
    "    test_scaled =  test.copy(deep=True)\n",
    "\n",
    "    #create list of numeric columns for scaling\n",
    "    num_cols = train.select_dtypes(include='number')\n",
    "\n",
    "    #fit to data\n",
    "    scaler.fit(num_cols)\n",
    "\n",
    "    # apply\n",
    "    train_scaled[num_cols.columns] = scaler.transform(train[num_cols.columns])\n",
    "    validate_scaled[num_cols.columns] =  scaler.transform(validate[num_cols.columns])\n",
    "    test_scaled[num_cols.columns] =  scaler.transform(test[num_cols.columns])\n",
    "\n",
    "    return train_scaled, validate_scaled, test_scaled\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prep_zillow (df):\n",
    "    \"\"\" \n",
    "    Purpose\n",
    "        Perform preparation functions on the zillow dataset\n",
    "    Parameters\n",
    "        df: data acquired from zillow dataset\n",
    "    Output\n",
    "        df: the unsplit and unscaled data with removed columns\n",
    "    \"\"\"\n",
    "    df = engineer_features(df)\n",
    "\n",
    "    df = df.drop(columns=['parcelid', 'buildingqualitytypeid','censustractandblock', 'calculatedbathnbr',\n",
    "                        'heatingorsystemtypeid', 'propertylandusetypeid', 'year_built', \n",
    "                        'rawcensustractandblock', 'landuse', 'fullbathcnt', 'finishedsquarefeet12',\n",
    "                        'assessmentyear', 'regionidcounty','regionidzip', 'regionidcity','tax_amount'])\n",
    "    df = df.set_index('id')\n",
    "\n",
    "    #fill na values\n",
    "    df.heating.fillna('None', inplace=True)\n",
    "    df.aircon.fillna('None', inplace=True)\n",
    "    df.basementsqft.fillna(0,inplace=True)\n",
    "    df.garagecarcnt.fillna(0,inplace=True)\n",
    "    df.garagetotalsqft.fillna(0,inplace=True)\n",
    "    df.unitcnt.fillna(1,inplace=True)\n",
    "    df.poolcnt.fillna(0, inplace=True)\n",
    "\n",
    "    #fix data types\n",
    "    col_to_fix = ['propertycountylandusecode',\n",
    "                'propertyzoningdesc']\n",
    "\n",
    "    for col in col_to_fix:\n",
    "        df[col] = df[col].astype('str')\n",
    "\n",
    "    # handle the missing data --> decisions made in advance\n",
    "    df = handle_missing_values(df, prop_required_columns=0.64)\n",
    "\n",
    "    # take care of unitcnts (more than 1 and was not nan when brought in)\n",
    "    df = df[df[\"unitcnt\"] == 1]\n",
    "\n",
    "    # take care of any duplicates:\n",
    "    df = df.drop_duplicates()\n",
    "\n",
    "    #drop na/duplicates --> adjust this for project. think of columns to impute\n",
    "    df = df.dropna()\n",
    "\n",
    "    df.drop(columns=\"unitcnt\",inplace=True)\n",
    "    \n",
    "    #split the data\n",
    "    train, validate, test = split_data(df)\n",
    "\n",
    "    #scale the data\n",
    "    train_scaled, validate_scaled, test_scaled = scale_split_data(train, validate, test)\n",
    "\n",
    "    return df, train, validate, test, train_scaled, validate_scaled, test_scaled     \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def wrangle_zillow():\n",
    "    \"\"\" \n",
    "    Purpose\n",
    "        Perform acuire and preparation functions on the zillow dataset\n",
    "    Parameters\n",
    "        None\n",
    "    Output\n",
    "        df: the unsplit and unscaled data\n",
    "        X_train:\n",
    "        X_train_scaled:\n",
    "        X_validate:\n",
    "        X_validate_scaled:\n",
    "        X_test:\n",
    "        X_test_scaled:\n",
    "    \"\"\"\n",
    "    #initial data acquisition\n",
    "    df = get_zillow_data()\n",
    "    \n",
    "    #drop columns that are unneeded, split data\n",
    "    df, train, validate, test, train_scaled, validate_scaled, test_scaled = prep_zillow(df)\n",
    "\n",
    "    #summarize the data\n",
    "    summarize(df)\n",
    "\n",
    "    return df, train, validate, test, train_scaled, validate_scaled, test_scaled  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !!!!!!!! WRITE UP A MODULE DESCRIPTION\n",
    "\n",
    "def get_connection(db, user=env.user, host=env.host, password=env.password):\n",
    "    return f'mysql+pymysql://{user}:{password}@{host}/{db}'\n",
    "\n",
    "def get_zillow_data():\n",
    "    '''\n",
    "    Reads in all fields from the customers table in the mall_customers schema from data.codeup.com\n",
    "    \n",
    "    parameters: None\n",
    "    \n",
    "    returns: a single Pandas DataFrame with the index set to the primary customer_id field\n",
    "    '''\n",
    "\n",
    "    sql = \"\"\"\n",
    "    SELECT \n",
    "        prop.*,\n",
    "        predictions_2017.logerror as logerror,\n",
    "        aircon.airconditioningdesc as aircon,\n",
    "        arch.architecturalstyledesc as architecture,\n",
    "        buildclass.buildingclassdesc as building_class, \n",
    "        heating.heatingorsystemdesc as heating,\n",
    "        landuse.propertylandusedesc as landuse, \n",
    "        story.storydesc as story,\n",
    "        construct_type.typeconstructiondesc as construct_type\n",
    "    FROM properties_2017 prop\n",
    "    JOIN (\n",
    "        SELECT parcelid, MAX(transactiondate) AS max_transactiondate\n",
    "        FROM predictions_2017\n",
    "        GROUP BY parcelid\n",
    "        ) pred USING (parcelid)\n",
    "    JOIN predictions_2017 ON pred.parcelid = predictions_2017.parcelid\n",
    "                        AND pred.max_transactiondate = predictions_2017.transactiondate\n",
    "    LEFT JOIN propertylandusetype landuse USING (propertylandusetypeid)\n",
    "    LEFT JOIN airconditioningtype aircon USING (airconditioningtypeid)\n",
    "    LEFT JOIN architecturalstyletype arch USING (architecturalstyletypeid)\n",
    "    LEFT JOIN buildingclasstype buildclass USING (buildingclasstypeid)\n",
    "    LEFT JOIN heatingorsystemtype heating USING (heatingorsystemtypeid)\n",
    "    LEFT JOIN storytype story USING (storytypeid)\n",
    "    LEFT JOIN typeconstructiontype construct_type USING (typeconstructiontypeid)\n",
    "    WHERE propertylandusedesc IN (\"Single Family Residential\", \"Inferred Single Family Residential\") \n",
    "        AND transactiondate like '%%2017%%';\n",
    "    \"\"\"\n",
    "\n",
    "    if exists('zillow_data.csv'):\n",
    "        df = pd.read_csv('zillow_data.csv')\n",
    "    else:\n",
    "        df = pd.read_sql(sql, get_connection('zillow'))\n",
    "        df.to_csv('zillow.csv', index=False)\n",
    "    return df\n",
    "\n",
    "def nulls_by_col(df):\n",
    "    num_missing = df.isnull().sum()\n",
    "    percnt_miss = num_missing / df.shape[0] * 100\n",
    "    cols_missing = pd.DataFrame(\n",
    "        {\n",
    "            'num_rows_missing': num_missing,\n",
    "            'percent_rows_missing': percnt_miss\n",
    "        }\n",
    "    )\n",
    "    return cols_missing\n",
    "\n",
    "def nulls_by_row(df):\n",
    "    num_missing = df.isnull().sum(axis=1)\n",
    "    prnt_miss = num_missing / df.shape[1] * 100\n",
    "    rows_missing = pd.DataFrame({'num_cols_missing': num_missing, 'percent_cols_missing': prnt_miss})\n",
    "    rows_missing = rows_missing.reset_index().groupby(['num_cols_missing', 'percent_cols_missing']).count().reset_index()\n",
    "\n",
    "    return rows_missing\n",
    "def engineer_features(df):\n",
    "    \"\"\"\n",
    "    \"\"\"\n",
    "\n",
    "    #remove unwanted columns, and reset index to id --> for the exercises\n",
    "    #age\n",
    "    df['age'] = 2022 - df['yearbuilt']\n",
    "\n",
    "    #log error bin\n",
    "    df['logerror_bin'] = pd.cut(df.logerror,[-6, df.logerror.mean() - df.logerror.std(), \n",
    "                            df.logerror.mean() + df.logerror.std(), 10],labels=['<-1sig','-1sig~1sig','>1sig'])\n",
    "    \n",
    "    #rename \n",
    "    df = df.rename(columns={'fips': 'county',\n",
    "                            'bedroomcnt': 'bedrooms', \n",
    "                            'bathroomcnt':'bathrooms', \n",
    "                            'calculatedfinishedsquarefeet': 'area', \n",
    "                            'taxvaluedollarcnt': 'home_value',\n",
    "                            'yearbuilt': 'year_built', \n",
    "                            'taxamount': 'tax_amount', \n",
    "                            })\n",
    "    \n",
    "    # #### Decades: \n",
    "    # #create list to hold labels for decades\n",
    "    # decade_labels = [x + 's' for x in np.arange(1870, 2030, 10)[:-1].astype('str')]\n",
    "\n",
    "    # #assign decades created from range to new decades column in dataset and apply labels\n",
    "    # df['decades'] = pd.cut(df.year_built, np.arange(1870, 2030, 10), labels=decade_labels, ordered=True)\n",
    "\n",
    "    #### Home Size\n",
    "    #use quantiles to calculate subgroups and assign to new column\n",
    "    q1, q3 = df.area.quantile([.25, .75])\n",
    "    df['home_size'] = pd.cut(df.area, [0,q1,q3, df.area.max()], labels=['small', 'medium', 'large'], right=True)\n",
    "\n",
    "    #### Estimated Tax Rate\n",
    "    df['est_tax_rate'] = df.tax_amount / df.home_value\n",
    "\n",
    "    df.county = df.county.map({6037: 'LA County', 6059: 'Orange County', 6111: 'Ventura County'})\n",
    "\n",
    "    return df\n",
    "def summarize(df):\n",
    "    print('-----')\n",
    "    print('DataFrame info:\\n')\n",
    "    print (df.info())\n",
    "    print('---')\n",
    "    print('DataFrame describe:\\n')\n",
    "    print (df.describe())\n",
    "    print('---')\n",
    "    print('DataFrame null value asssessment:\\n')\n",
    "    print('Nulls By Column:', nulls_by_col(df))\n",
    "    print('----')\n",
    "    print('Nulls By Row:', nulls_by_row(df))\n",
    "    numerical_cols = df.select_dtypes(include='number').columns.to_list()\n",
    "    categorical_cols = df.select_dtypes(exclude='number').columns.to_list()\n",
    "    print('value_counts: \\n')\n",
    "    for col in df.columns:\n",
    "        print(f'Column Names: {col}')\n",
    "        if col in categorical_cols:\n",
    "            print(df[col].value_counts())\n",
    "        else:\n",
    "            print(df[col].value_counts(bins=10, sort=False, dropna=False))\n",
    "            print('---')\n",
    "    print('Report Finished')\n",
    "    return\n",
    "\n",
    "\n",
    "def handle_missing_values(df, prop_required_columns=0.60, prop_required_row=0.75):\n",
    "    threshold = int(round(prop_required_columns * len(df.index), 0))\n",
    "    df = df.dropna(axis=1, thresh=threshold)\n",
    "    threshold = int(round(prop_required_row * len(df.columns), 0))\n",
    "    df = df.dropna(axis=0, thresh=threshold)\n",
    "\n",
    "    return df\n",
    "\n",
    "\n",
    "def split_data(df):\n",
    "    train_validate, test = train_test_split(df, test_size= .2, random_state=514)\n",
    "    train, validate = train_test_split(train_validate, test_size= .3, random_state=514)\n",
    "    print(train.shape, validate.shape, test.shape)\n",
    "    return train, validate, test\n",
    "\n",
    "\n",
    "def scale_split_data (train, validate, test):\n",
    "    #create scaler object\n",
    "    scaler = MinMaxScaler()\n",
    "\n",
    "    # create copies to hold scaled data\n",
    "    train_scaled = train.copy(deep=True)\n",
    "    validate_scaled = validate.copy(deep=True)\n",
    "    test_scaled =  test.copy(deep=True)\n",
    "\n",
    "    #create list of numeric columns for scaling\n",
    "    num_cols = train.select_dtypes(include='number')\n",
    "\n",
    "    #fit to data\n",
    "    scaler.fit(num_cols)\n",
    "\n",
    "    # apply\n",
    "    train_scaled[num_cols.columns] = scaler.transform(train[num_cols.columns])\n",
    "    validate_scaled[num_cols.columns] =  scaler.transform(validate[num_cols.columns])\n",
    "    test_scaled[num_cols.columns] =  scaler.transform(test[num_cols.columns])\n",
    "\n",
    "    return train_scaled, validate_scaled, test_scaled\n",
    "\n",
    "\n",
    "def prep_zillow (df):\n",
    "    \"\"\" \n",
    "    Purpose\n",
    "        Perform preparation functions on the zillow dataset\n",
    "    Parameters\n",
    "        df: data acquired from zillow dataset\n",
    "    Output\n",
    "        df: the unsplit and unscaled data with removed columns\n",
    "    \"\"\"\n",
    "    df = engineer_features(df)\n",
    "\n",
    "    df = df.drop(columns=['parcelid', 'buildingqualitytypeid','censustractandblock', 'calculatedbathnbr',\n",
    "                        'heatingorsystemtypeid', 'propertylandusetypeid', 'year_built', \n",
    "                        'rawcensustractandblock', 'landuse', 'fullbathcnt', 'finishedsquarefeet12',\n",
    "                        'assessmentyear', 'regionidcounty','regionidzip', 'regionidcity','tax_amount'])\n",
    "    df = df.set_index('id')\n",
    "\n",
    "    #fill na values\n",
    "    df.heating.fillna('None', inplace=True)\n",
    "    df.aircon.fillna('None', inplace=True)\n",
    "    df.basementsqft.fillna(0,inplace=True)\n",
    "    df.garagecarcnt.fillna(0,inplace=True)\n",
    "    df.garagetotalsqft.fillna(0,inplace=True)\n",
    "    df.unitcnt.fillna(1,inplace=True)\n",
    "    df.poolcnt.fillna(0, inplace=True)\n",
    "\n",
    "    #fix data types\n",
    "    col_to_fix = ['propertycountylandusecode',\n",
    "                'propertyzoningdesc']\n",
    "\n",
    "    for col in col_to_fix:\n",
    "        df[col] = df[col].astype('str')\n",
    "\n",
    "    # handle the missing data --> decisions made in advance\n",
    "    df = handle_missing_values(df, prop_required_columns=0.64)\n",
    "\n",
    "    # take care of unitcnts (more than 1 and was not nan when brought in)\n",
    "    df = df[df[\"unitcnt\"] == 1]\n",
    "\n",
    "    # take care of any duplicates:\n",
    "    df = df.drop_duplicates()\n",
    "\n",
    "    #drop na/duplicates --> adjust this for project. think of columns to impute\n",
    "    df = df.dropna()\n",
    "\n",
    "    df.drop(columns=\"unitcnt\",inplace=True)\n",
    "    \n",
    "    #split the data\n",
    "    train, validate, test = split_data(df)\n",
    "\n",
    "    #scale the data\n",
    "    train_scaled, validate_scaled, test_scaled = scale_split_data(train, validate, test)\n",
    "\n",
    "    return df, train, validate, test, train_scaled, validate_scaled, test_scaled     \n",
    "\n",
    "\n",
    "def wrangle_zillow():\n",
    "    \"\"\" \n",
    "    Purpose\n",
    "        Perform acuire and preparation functions on the zillow dataset\n",
    "    Parameters\n",
    "        None\n",
    "    Output\n",
    "        df: the unsplit and unscaled data\n",
    "        X_train:\n",
    "        X_train_scaled:\n",
    "        X_validate:\n",
    "        X_validate_scaled:\n",
    "        X_test:\n",
    "        X_test_scaled:\n",
    "    \"\"\"\n",
    "    #initial data acquisition\n",
    "    df = get_zillow_data()\n",
    "    \n",
    "    #drop columns that are unneeded, split data\n",
    "    df, train, validate, test, train_scaled, validate_scaled, test_scaled = prep_zillow(df)\n",
    "\n",
    "    #summarize the data\n",
    "    summarize(df)\n",
    "\n",
    "    return df, train, validate, test, train_scaled, validate_scaled, test_scaled  \n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.12 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "38cca0c38332a56087b24af0bc80247f4fced29cb4f7f437d91dc159adec9c4e"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
